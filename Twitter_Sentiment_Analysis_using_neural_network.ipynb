{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hastikacheddy/Base_ML_Notebooks/blob/main/Twitter_Sentiment_Analysis_using_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "92b885dd147dac19bd0a33db3cd0da100bd5bc23",
        "id": "orEmtt1oOTQo"
      },
      "cell_type": "markdown",
      "source": [
        "# Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gensim:\n",
        "\n",
        "Purpose:\n",
        "\n",
        "Gensim is a Python library designed for topic modeling, document indexing, and similarity retrieval with large corpora. It's widely used for text processing and natural language processing tasks.\n",
        "\n",
        "Features:\n",
        "\n",
        "*   Topic modeling algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA).\n",
        "*   Similarity queries, including cosine similarity.\n",
        "\n",
        "*   Document indexing and retrieval.\n",
        "*   Word vector models like Word2Vec, FastText, and more.\n",
        "\n",
        "\n",
        "### keras:\n",
        "\n",
        "Purpose: Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It simplifies the process of building and training deep learning models.\n",
        "\n",
        "Features:\n",
        "\n",
        "*   Provides a simple and intuitive interface for building neural networks.\n",
        "*   Supports convolutional networks, recurrent networks, and combinations of both.\n",
        "\n",
        "*   Enables fast experimentation through a user-friendly API.\n",
        "*   Integrates seamlessly with TensorFlow, which allows for easy scaling to distributed systems.\n",
        "\n",
        "\n",
        "### pandas:\n",
        "\n",
        "Purpose: Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrame and Series that are flexible and easy to work with, making it a popular choice for data cleaning, transformation, and analysis tasks.\n",
        "\n",
        "Features:\n",
        "\n",
        "*   DataFrame object for data manipulation with integrated indexing.\n",
        "Tools for reading and writing data in various formats (CSV, Excel, SQL databases, etc.).\n",
        "*   Time series functionality for working with date and time data.\n",
        "\n",
        "*   Powerful data aggregation and transformation capabilities.\n",
        "*   Integration with other libraries like NumPy and Matplotlib for scientific computing and data visualization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DSfqGS4GO_VI"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70282bce8b42a51e4d44f2c7d85c4ca9567b0fd4",
        "id": "p4TjhBtGOTQp"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim --upgrade\n",
        "!pip install keras --upgrade\n",
        "!pip install pandas --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "303e72966af732ddef0bd8108a321095314e44af",
        "id": "ozDQcPhxOTQp"
      },
      "cell_type": "code",
      "source": [
        "# Importing pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Importing matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  # Magic command to display plots inline in Jupyter Notebook\n",
        "\n",
        "# Importing scikit-learn modules for machine learning tasks\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "from sklearn.preprocessing import LabelEncoder       # For label encoding categorical variables\n",
        "from sklearn.metrics import (                         # For evaluating model performance\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.manifold import TSNE                     # For data visualization and dimensionality reduction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For text vectorization using TF-IDF\n",
        "\n",
        "# Importing Keras modules for deep learning\n",
        "from keras.preprocessing.text import Tokenizer       # For text tokenization\n",
        "from keras.preprocessing.sequence import pad_sequences  # For padding sequences to a fixed length\n",
        "from keras.models import Sequential                  # For building sequential neural network models\n",
        "from keras.layers import (                            # Different types of neural network layers\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    Conv1D,\n",
        "    MaxPooling1D,\n",
        "    LSTM\n",
        ")\n",
        "from keras import utils                              # Utilities for data manipulation in Keras\n",
        "from keras.callbacks import (                        # Callbacks for model training\n",
        "    ReduceLROnPlateau,\n",
        "    EarlyStopping\n",
        ")\n",
        "\n",
        "# Importing nltk for natural language processing tasks\n",
        "import nltk\n",
        "from nltk.corpus import stopwords                    # Stopwords list for text preprocessing\n",
        "from nltk.stem import SnowballStemmer                # Stemmer for text preprocessing\n",
        "\n",
        "# Importing gensim for word embedding models\n",
        "import gensim\n",
        "\n",
        "# Importing utility modules\n",
        "import re                                            # Regular expression operations\n",
        "import numpy as np                                   # Numerical operations\n",
        "import os                                            # Operating system dependent functionality\n",
        "from collections import Counter                      # Counter for counting occurrences\n",
        "import logging                                       # Logging for tracking progress\n",
        "import time                                          # Time-related functions\n",
        "import pickle                                        # Serialization and deserialization of Python objects\n",
        "import itertools                                     # Functions for efficient looping\n",
        "\n",
        "# Setting up logging format\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different types of layers that use to construct neural network architectures in Keras. Here's a brief explanation of each:\n",
        "\n",
        "1. **Activation**:\n",
        "   - **Purpose**: Specifies the activation function to be used in a neural network layer.\n",
        "   - **Usage**: Typically used after a layer to introduce non-linearity into the model. Common activation functions include ReLU, sigmoid, and tanh.\n",
        "\n",
        "2. **Dense**:\n",
        "   - **Purpose**: A fully connected layer where each neuron is connected to every neuron in the previous layer.\n",
        "   - **Usage**: Often used as the output layer or in hidden layers of a neural network.\n",
        "\n",
        "3. **Dropout**:\n",
        "   - **Purpose**: A regularization technique to prevent overfitting in neural networks.\n",
        "   - **Usage**: Randomly sets a fraction of input units to 0 during training to reduce the dependency on certain neurons.\n",
        "\n",
        "4. **Embedding**:\n",
        "   - **Purpose**: Represents categorical data or discrete entities as continuous vectors.\n",
        "   - **Usage**: Commonly used in natural language processing tasks to convert word indices into dense vectors.\n",
        "\n",
        "5. **Flatten**:\n",
        "   - **Purpose**: Reshapes the input tensor into a 1D tensor.\n",
        "   - **Usage**: Typically used to flatten the output of convolutional layers before feeding it into fully connected layers.\n",
        "\n",
        "6. **Conv1D**:\n",
        "   - **Purpose**: 1D convolutional layer for processing sequential data.\n",
        "   - **Usage**: Used in neural networks for tasks like text classification and time series forecasting.\n",
        "\n",
        "7. **MaxPooling1D**:\n",
        "   - **Purpose**: Downsamples the input along the temporal dimension.\n",
        "   - **Usage**: Used to reduce the spatial dimensions of the input, making the network more computationally efficient and reducing overfitting.\n",
        "\n",
        "8. **LSTM**:\n",
        "   - **Purpose**: Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture.\n",
        "   - **Usage**: Particularly effective for handling sequential data due to its ability to capture long-term dependencies.\n"
      ],
      "metadata": {
        "id": "b72MwI3dSSE-"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35e1a89dead5fd160e4c9a024a21d2e569fc89ff",
        "id": "YE6SHLxROTQp"
      },
      "cell_type": "code",
      "source": [
        "# Downloading the stopwords corpus from NLTK for text preprocessing\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e8b01a07df001e4abcc745900336c4db06e455f3",
        "id": "7ZoTHqW-OTQq"
      },
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "180f0dd2a95419e4602b5c0229822b0111c826f6",
        "id": "0an-1gr6OTQq"
      },
      "cell_type": "code",
      "source": [
        "# DATASET\n",
        "# Define the column names for the dataset\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "# Define the encoding of the dataset file\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "# Define the size of the training set as a fraction of the total dataset\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLEANING\n",
        "# Define the regular expression pattern for text cleaning\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC\n",
        "# Define the dimensionality of the word vectors\n",
        "W2V_SIZE = 300\n",
        "# Define the size of the context window for word2vec\n",
        "W2V_WINDOW = 7\n",
        "# Define the number of training epochs for word2vec\n",
        "W2V_EPOCH = 32\n",
        "# Define the minimum count threshold for words in word2vec\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "# Define the maximum length of input sequences for Keras\n",
        "SEQUENCE_LENGTH = 300\n",
        "# Define the number of training epochs for Keras\n",
        "EPOCHS = 8\n",
        "# Define the batch size for training in Keras\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "# Define labels for sentiment analysis\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "# Define thresholds for classifying sentiments\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "# Define filenames for saving trained models and tokenizer\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KERAS_MODEL = \"model.h5\":\n",
        "\n",
        "Purpose: This filename is used to save the trained Keras model.\n",
        "File Extension: .h5 is a common extension used to save Keras models in the Hierarchical Data Format (HDF5), which is a data model, library, and file format for storing and managing large amounts of data.\n",
        "\n",
        "WORD2VEC_MODEL = \"model.w2v\":\n",
        "\n",
        "Purpose: This filename is used to save the trained Word2Vec model.\n",
        "File Extension: .w2v is a custom extension you are using to denote that the file contains a Word2Vec model. However, Word2Vec models are typically saved using .bin or .txt extensions.\n",
        "\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\":\n",
        "\n",
        "Purpose: This filename is used to save the tokenizer object.\n",
        "File Extension: .pkl stands for \"pickle\", which is a module in Python used for serializing and deserializing Python object structures. The tokenizer object contains the vocabulary and other configurations used to preprocess text data.\n",
        "\n",
        "ENCODER_MODEL = \"encoder.pkl\":\n",
        "\n",
        "Purpose: This filename is used to save the label encoder object.\n",
        "File Extension: Like the tokenizer, the encoder is also saved with a .pkl extension using the pickle module. The encoder is used to convert categorical labels into numerical format, which is necessary for training machine learning models."
      ],
      "metadata": {
        "id": "7NDRhs4ZTLJY"
      }
    },
    {
      "metadata": {
        "_uuid": "1c3beecc618be68480b3d4f0de08d9d863da1dc1",
        "id": "kXMOj0tOOTQq"
      },
      "cell_type": "markdown",
      "source": [
        "### Read Dataset"
      ]
    },
    {
      "metadata": {
        "_uuid": "563b3c44f1092dba0b853747b098e00509098cca",
        "id": "GFZhsjFwOTQr"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset details\n",
        "* **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "* **ids**: The id of the tweet ( 2087)\n",
        "* **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "* **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "* **user**: the user that tweeted (robotickilldozr)\n",
        "* **text**: the text of the tweet (Lyx is cool)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bba8f91cd70de4f5ea0fb0870ae2029b6e3dcc24",
        "id": "Co0RYqA5OTQr"
      },
      "cell_type": "code",
      "source": [
        "import os  # Importing the os module for interacting with the operating system\n",
        "import pandas as pd  # Importing pandas for data manipulation\n",
        "\n",
        "# Get the list of filenames in the \"../input\" directory and select the first one\n",
        "dataset_filename = os.listdir(\"../input\")[0]\n",
        "\n",
        "# Construct the full path to the selected dataset file\n",
        "dataset_path = os.path.join(\"..\", \"input\", dataset_filename)\n",
        "\n",
        "# Print the path of the file that will be opened\n",
        "print(\"Open file:\", dataset_path)\n",
        "\n",
        "# Read the dataset file into a DataFrame using pandas\n",
        "# - `encoding=DATASET_ENCODING`: Specify the encoding of the file\n",
        "# - `names=DATASET_COLUMNS`: Specify the column names for the DataFrame\n",
        "df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "936d499c00c4f1648bc16ca9d283c3b39be7fb10",
        "id": "PAYXY9jWOTQs"
      },
      "cell_type": "code",
      "source": [
        "print(\"Dataset size:\", len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7486ed895b813c5246f97b31b6162b0f65ff763b",
        "id": "ykk2TRWSOTQs"
      },
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f9a7bb129e184967b13261fb5d253af451c75c5",
        "id": "TMGvQhw5OTQs"
      },
      "cell_type": "markdown",
      "source": [
        "### Map target label to String\n",
        "* **0** -> **NEGATIVE**\n",
        "* **2** -> **NEUTRAL**\n",
        "* **4** -> **POSITIVE**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14074b59106cb9550440839e48b832223fc9502f",
        "id": "hUiW9fDROTQs"
      },
      "cell_type": "code",
      "source": [
        "# Define a mapping dictionary to map numerical labels to sentiment categories\n",
        "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "\n",
        "# Define a function to decode numerical labels to sentiment categories\n",
        "def decode_sentiment(label):\n",
        "    # Convert the label to an integer and use it to look up the corresponding sentiment category in the decode_map\n",
        "    return decode_map[int(label)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4449d473187f647a195a6ac6986b009da32a7f4b",
        "id": "wb5nSoXDOTQt"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Measure the time taken to execute the following code block\n",
        "\n",
        "# Use the decode_sentiment function to decode numerical sentiment labels in the 'target' column\n",
        "df.target = df.target.apply(lambda x: decode_sentiment(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19eb327803192f31cce3512aacb232f4d6b38715",
        "id": "LljimRY9OTQt"
      },
      "cell_type": "code",
      "source": [
        "# Count the frequency of each unique target label in the DataFrame\n",
        "target_cnt = Counter(df.target)\n",
        "\n",
        "# Create a new figure for plotting\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "# Create a bar plot to visualize the distribution of target labels\n",
        "plt.bar(target_cnt.keys(), target_cnt.values())\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Dataset labels distribution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4329b1573518b03e497213efa7676220734ebb4b",
        "id": "6eKn97UwOTQt"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-Process dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8aeee8b7b9ea11b749c7f91cd4787a7b50ed1a91",
        "id": "bBezE-XSOTQt"
      },
      "cell_type": "code",
      "source": [
        "# Get the list of stopwords for the English language from nltk\n",
        "# Stopwords are common words (such as \"and\", \"the\", \"is\", etc.) that are often filtered out during text preprocessing in natural language processing tasks. The resulting list is stored in the stop_words variable.\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "# Create a Snowball stemmer object for English\n",
        "# Stemming is the process of reducing words to their root or base form. The resulting stemmer object is stored in the stemmer variable.\n",
        "stemmer = SnowballStemmer(\"english\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "649ebcb97969b9ac4301138783704bb3d7846a49",
        "id": "D9QDekLOOTQt"
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text, stem=False):\n",
        "    # Remove links, user mentions, and special characters from the text\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "\n",
        "    # Initialize an empty list to store the tokens after preprocessing\n",
        "    tokens = []\n",
        "\n",
        "    # Iterate through each token in the text\n",
        "    for token in text.split():\n",
        "        # Check if the token is not in the list of stopwords\n",
        "        if token not in stop_words:\n",
        "            # Check if stemming is required\n",
        "            if stem:\n",
        "                # Stem the token using the Snowball stemmer\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                # Otherwise, add the token to the list of tokens\n",
        "                tokens.append(token)\n",
        "\n",
        "    # Join the list of tokens into a single string and return it\n",
        "    return \" \".join(tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7f3e77ab9291d14687c49e71ba9b2b1e3323432",
        "id": "fzJa0PPaOTQt"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Measure the time taken to execute the following code block\n",
        "\n",
        "# Apply the preprocess function to each text entry in the 'text' column of the DataFrame\n",
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5f9714a8507409bbe780eebf2855a33e8e6ba37",
        "id": "BqnXhjrXOTQt"
      },
      "cell_type": "markdown",
      "source": [
        "### Split train and test"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2b1179c968e3f3910c790ecf0c5b2cbb34b0e68",
        "id": "eU5fRYC6OTQt"
      },
      "cell_type": "code",
      "source": [
        "# Split the DataFrame into training and testing sets\n",
        "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "\n",
        "# Print the sizes of the training and testing sets\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f08a28aab2c3d16d8b9681a7d5d07587153a1cd6",
        "id": "D0Jzv1GcOTQt"
      },
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2461bf564de1b4414841933d0c1d1bee5f5cc5a6",
        "id": "4kBMUYMUOTQt"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Measure the time taken to execute the following code block\n",
        "\n",
        "# Split each text entry in the 'text' column of the training DataFrame into a list of tokens\n",
        "documents = [_text.split() for _text in df_train.text]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e19b9f25801ba86420decc266d2b3e6fb44f1ea",
        "id": "AFVOxetpOTQt"
      },
      "cell_type": "code",
      "source": [
        "# Initialize a Word2Vec model using the parameters specified\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "    size=W2V_SIZE,          # Dimensionality of the word vectors\n",
        "    window=W2V_WINDOW,      # Maximum distance between the current and predicted word within a sentence\n",
        "    min_count=W2V_MIN_COUNT,  # Minimum frequency count of words to consider\n",
        "    workers=8               # Number of CPU cores to use for training the model\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this code is to construct the vocabulary for the Word2Vec model using the tokenized text data. Building the vocabulary involves identifying unique words (tokens) in the documents and assigning an index to each word, which will be used during the training phase to learn word embeddings."
      ],
      "metadata": {
        "id": "bPy1UixwVh4c"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58d655af07653c594bec6bebcfb302a973b0ad9c",
        "id": "KbQKhwMGOTQu"
      },
      "cell_type": "code",
      "source": [
        "# Build the vocabulary of the Word2Vec model using the tokenized documents\n",
        "w2v_model.build_vocab(documents)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72a5628ca81fd4b8983c12d93ae0bf950b86b6ae",
        "id": "tVBKpvGpOTQu"
      },
      "cell_type": "code",
      "source": [
        "# Retrieve the keys (words) from the Word2Vec model's vocabulary\n",
        "words = w2v_model.wv.vocab.keys()\n",
        "\n",
        "# Calculate the size of the vocabulary\n",
        "vocab_size = len(words)\n",
        "\n",
        "# Print the size of the vocabulary\n",
        "print(\"Vocab size\", vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68c3e4a5ba07cac3dee67f78ecdd1404c7f83f14",
        "id": "Sg_eqFBaOTQu"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Measure the time taken to execute the following code block\n",
        "\n",
        "# Train the Word2Vec model on the tokenized documents\n",
        "w2v_model.train(\n",
        "    documents,                    # Tokenized documents\n",
        "    total_examples=len(documents), # Total number of documents\n",
        "    epochs=W2V_EPOCH               # Number of training epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27cc2651c74227115d8bfd8c40e5618048e05edd",
        "id": "7_EXV80oOTQu"
      },
      "cell_type": "code",
      "source": [
        "w2v_model.most_similar(\"love\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e13563644468037258598637b49373ca96b9b879",
        "id": "VrsI1tVROTQu"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize Text"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6852bc709a7cd20173cbeeb218505078f8f37c57",
        "id": "86cAmrs_OTQu"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Measure the time taken to execute the following code block\n",
        "\n",
        "# Initialize a Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the Tokenizer on the text data in the training set\n",
        "tokenizer.fit_on_texts(df_train.text)\n",
        "\n",
        "# Calculate the total number of unique words in the vocabulary\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Print the total number of unique words in the vocabulary\n",
        "print(\"Total words\", vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text data into integers is a necessary step in preparing the data for training machine learning models, particularly neural networks. Here are a few reasons why this conversion is important:\n",
        "\n",
        "Numerical Representation: Machine learning models, including neural networks, require numerical input. Text data, being categorical in nature, needs to be converted into a numerical format that can be processed by these models.\n",
        "\n",
        "Fixed Length Input: Neural networks require fixed-size input vectors. Tokenizing and converting text into integers allows you to represent variable-length text sequences as fixed-length integer sequences by padding or truncating as needed.\n",
        "\n",
        "Efficient Storage and Computation: Integer representations are more memory-efficient and faster to process compared to raw text data. This is crucial when dealing with large datasets and complex models.\n",
        "\n",
        "Embedding Layer in Neural Networks: In natural language processing tasks, such as text classification or sentiment analysis, the tokenized integer sequences are often passed through an embedding layer in a neural network. This layer converts the integer indices into dense vectors (embeddings) of fixed size, where semantically similar words are mapped to nearby points in the vector space.\n",
        "\n",
        "Consistency: Converting text into a consistent numerical format ensures that the model receives standardized input, which is essential for achieving reliable and consistent performance."
      ],
      "metadata": {
        "id": "QLEoOCG-WJTG"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45de439df3015030c71f84c2d170346936a1d68f",
        "id": "9wGS3txnOTQu"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Measure the time taken to execute the following code block\n",
        "\n",
        "# Convert the tokenized text sequences into padded sequences of integers for training and testing sets\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pad_sequences function is used to ensure that all sequences in a list have the same length by either padding or truncating them. This is particularly important when working with sequences of variable length, such as sentences or paragraphs of text, which need to be converted into fixed-length vectors for input to machine learning models like neural networks."
      ],
      "metadata": {
        "id": "0_vdWTGpWYfn"
      }
    },
    {
      "metadata": {
        "_uuid": "03b35903fc6260e190d6928d240ef7432de117fc",
        "id": "b2rV7gH5OTQv"
      },
      "cell_type": "markdown",
      "source": [
        "### Label Encoder"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33676e0efa39e97d89bd650b8b4eae933a22fbf0",
        "id": "cVyM2CoiOTQv"
      },
      "cell_type": "code",
      "source": [
        "# Get the unique labels from the 'target' column of the training DataFrame\n",
        "labels = df_train.target.unique().tolist()\n",
        "\n",
        "# Append the 'NEUTRAL' label to the list of unique labels\n",
        "labels.append(NEUTRAL)\n",
        "\n",
        "# Print the updated list of labels\n",
        "print(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04239a9bef76e7922fd86098a5601dfde8ee4665",
        "id": "6U1DFpqxOTQv"
      },
      "cell_type": "code",
      "source": [
        "# Initialize a LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the 'target' column of the training DataFrame and transform the labels to integers\n",
        "y_train = encoder.fit_transform(df_train.target.tolist())\n",
        "y_test = encoder.transform(df_test.target.tolist())\n",
        "\n",
        "# Reshape the encoded labels to be 2D arrays\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Print the shapes of the encoded label arrays\n",
        "print(\"y_train\", y_train.shape)\n",
        "print(\"y_test\", y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04299c886911ca135583ab64878f213939a2990c",
        "id": "OjqI4MGfOTQv"
      },
      "cell_type": "code",
      "source": [
        "print(\"x_train\", x_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print()\n",
        "print(\"x_test\", x_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "232533fb27b7be99d9b8c2f8fb22c9c6bf121a6f",
        "id": "TK-5wy91OTQv"
      },
      "cell_type": "code",
      "source": [
        "y_train[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "233c0ea94055a03e2e7df3e2a13d036ec963484f",
        "id": "DyIMGGoBOTQw"
      },
      "cell_type": "markdown",
      "source": [
        "### Embedding layer"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ab488374b59e3f30f8b1ea92767d853c4846bac",
        "id": "eiaR5tL9OTQw"
      },
      "cell_type": "code",
      "source": [
        "# Initialize an embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "\n",
        "# Populate the embedding matrix with word vectors from the trained Word2Vec model\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Print the shape of the embedding matrix\n",
        "print(embedding_matrix.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "An embedding matrix is a 2D matrix where each row corresponds to a word in the vocabulary, and each column to a feature of the word vectors, typically from a pre-trained model like Word2Vec. It provides dense, fixed-size representations of words, aiding neural networks in processing text data."
      ],
      "metadata": {
        "id": "4WSDoEf-XD_r"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "833279d91e4286065968237fb5f2a0c2dd4d246c",
        "id": "U4HFCeexOTQz"
      },
      "cell_type": "code",
      "source": [
        "# Define an Embedding layer for the neural network model\n",
        "# This layer uses pre-trained Word2Vec embeddings as initial weights\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    vocab_size,                   # Size of the vocabulary\n",
        "    W2V_SIZE,                     # Dimensionality of the Word2Vec word vectors\n",
        "    weights=[embedding_matrix],   # Pre-trained embedding matrix as initial weights\n",
        "    input_length=SEQUENCE_LENGTH, # Length of input sequences\n",
        "    trainable=False               # Freeze the embedding layer during training\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b299ef78f94c2085942c993a2d58753a7476305a",
        "id": "3p0OgKVYOTQz"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e775ef4f1b74e6412457181383c39f2df554ef3f",
        "id": "Zxx4J6SmOTQz"
      },
      "cell_type": "code",
      "source": [
        "# Initialize a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the pre-trained embedding layer to the model\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Add a Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add an LSTM layer with 100 units\n",
        "# The dropout and recurrent dropout are used for regularization to prevent overfitting\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add a Dense output layer with a sigmoid activation function\n",
        "# This layer outputs a single value between 0 and 1, representing the predicted sentiment score\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28d22eafd0c7d798dcf3d742bc92fb8577939e6c",
        "id": "7xvYwe2WOTQz"
      },
      "cell_type": "markdown",
      "source": [
        "### Compile model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1331e08d590bb2aa2033706c8faca217afc0f1c3",
        "id": "Vle8NPKKOTQz"
      },
      "cell_type": "code",
      "source": [
        "# Compile the model with binary cross-entropy loss, Adam optimizer, and accuracy metric\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', # Loss function for binary classification\n",
        "    optimizer=\"adam\",            # Optimizer algorithm for model training\n",
        "    metrics=['accuracy']         # Evaluation metric to monitor during training\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.compile: This method configures the model for training.\n",
        "\n",
        "loss='binary_crossentropy': Specifies the loss function to use for binary classification tasks. Binary cross-entropy is commonly used for binary classification problems, like sentiment analysis.\n",
        "\n",
        "optimizer=\"adam\": Specifies the optimizer algorithm to use during training. Adam is an adaptive learning rate optimization algorithm that is well-suited for training deep learning models.\n",
        "\n",
        "metrics=['accuracy']: Specifies the evaluation metric to monitor during training. In this case, we are monitoring the classification accuracy of the model on the training and validation data."
      ],
      "metadata": {
        "id": "CvMFKIdlXlNP"
      }
    },
    {
      "metadata": {
        "_uuid": "c7733127cb8b380e0c807268903bf4d03ef92542",
        "id": "AtHUsBDgOTQz"
      },
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a688df590386f5748da6fe00b01904fe6c71619e",
        "id": "FGgR_1tcOTQz"
      },
      "cell_type": "code",
      "source": [
        "# Define callbacks to enhance training performance\n",
        "callbacks = [\n",
        "    # Reduce learning rate when a metric has stopped improving\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',    # Monitor validation loss\n",
        "        patience=5,             # Number of epochs with no improvement after which learning rate will be reduced\n",
        "        cooldown=0              # Number of epochs to wait before resuming normal operation after reducing the learning rate\n",
        "    ),\n",
        "    # Stop training when a monitored metric has stopped improving\n",
        "    EarlyStopping(\n",
        "        monitor='val_acc',      # Monitor validation accuracy\n",
        "        min_delta=1e-4,         # Minimum change in the monitored quantity to qualify as an improvement\n",
        "        patience=5              # Number of epochs with no improvement after which training will be stopped\n",
        "    )\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d0873633dd49179c8cae17377641b97d323ef3b",
        "id": "XPSpyS97OTQz"
      },
      "cell_type": "markdown",
      "source": [
        "### Train"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b659d390c6577dc5cdb6b6297934279b4e801d5",
        "id": "34_Kn5_WOTQz"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train the model and measure the training time\n",
        "history = model.fit(\n",
        "    x_train,                         # Input features (training data)\n",
        "    y_train,                         # Target labels (training data)\n",
        "    batch_size=BATCH_SIZE,           # Number of samples per gradient update\n",
        "    epochs=EPOCHS,                   # Number of training epochs\n",
        "    validation_split=0.1,            # Fraction of training data to be used as validation data\n",
        "    verbose=1,                       # Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch)\n",
        "    callbacks=callbacks              # Callbacks for enhancing training performance\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "267258196d96796ac69a7b8c466314bcf5d6ee42",
        "id": "8tesPXjdOTQz"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98ecd8f1b8b74594c3ea775dd68a094e92458022",
        "id": "M3xeTUnnOTQz"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Evaluate the model on the test data and measure the evaluation time\n",
        "score = model.evaluate(\n",
        "    x_test,                         # Input features (test data)\n",
        "    y_test,                         # Target labels (test data)\n",
        "    batch_size=BATCH_SIZE           # Number of samples per gradient update\n",
        ")\n",
        "\n",
        "# Print the evaluation results\n",
        "print()\n",
        "print(\"ACCURACY:\", score[1])        # Print the accuracy score\n",
        "print(\"LOSS:\", score[0])             # Print the loss value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40c72cd1e9d6c4fd799cbba7c813765ac4039dfc",
        "id": "_-ohFlckOTQz"
      },
      "cell_type": "code",
      "source": [
        "# Extract training history data\n",
        "acc = history.history['acc']           # Training accuracy\n",
        "val_acc = history.history['val_acc']   # Validation accuracy\n",
        "loss = history.history['loss']         # Training loss\n",
        "val_loss = history.history['val_loss'] # Validation loss\n",
        "\n",
        "# Create range of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')      # Training accuracy curve\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc') # Validation accuracy curve\n",
        "plt.title('Training and validation accuracy')         # Plot title\n",
        "plt.legend()                                           # Add legend\n",
        "plt.xlabel('Epochs')                                  # Label for x-axis\n",
        "plt.ylabel('Accuracy')                                # Label for y-axis\n",
        "plt.show()                                             # Show plot\n",
        "\n",
        "# Create new figure for Loss curves\n",
        "plt.figure()\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')      # Training loss curve\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss') # Validation loss curve\n",
        "plt.title('Training and validation loss')               # Plot title\n",
        "plt.legend()                                             # Add legend\n",
        "plt.xlabel('Epochs')                                    # Label for x-axis\n",
        "plt.ylabel('Loss')                                      # Label for y-axis\n",
        "plt.show()                                               # Show plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bdfc0f6a6af5bebc0271d83dd7432c91001409b",
        "id": "SnzuKnIwOTQz"
      },
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0b0fa3d4b1bb14b3f5e3d169a369f3ebef29ae1",
        "id": "LHAb-3aYOTQz"
      },
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score, include_neutral=True):\n",
        "    \"\"\"\n",
        "    Decode sentiment score to sentiment label\n",
        "    :param score: float, sentiment score between 0 and 1\n",
        "    :param include_neutral: bool, whether to include neutral sentiment or not\n",
        "    :return: str, sentiment label (POSITIVE, NEGATIVE, NEUTRAL)\n",
        "    \"\"\"\n",
        "    if include_neutral:\n",
        "        # Set default label to NEUTRAL\n",
        "        label = NEUTRAL\n",
        "        # Check if score is below negative threshold\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        # Check if score is above positive threshold\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        # Return NEGATIVE for scores below 0.5, otherwise POSITIVE\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed4086d651f2f8cbed11d3c909a8873607d29a06",
        "id": "uvKqQE_MOTQ0"
      },
      "cell_type": "code",
      "source": [
        "def predict(text, include_neutral=True):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a given text\n",
        "    :param text: str, input text for sentiment prediction\n",
        "    :param include_neutral: bool, whether to include neutral sentiment or not\n",
        "    :return: dict, dictionary containing predicted label, score, and elapsed time\n",
        "    \"\"\"\n",
        "    # Record the start time\n",
        "    start_at = time.time()\n",
        "\n",
        "    # Tokenize and pad the input text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "\n",
        "    # Predict sentiment score\n",
        "    score = model.predict([x_test])[0]\n",
        "\n",
        "    # Decode the sentiment score to label\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    # Calculate elapsed time for prediction\n",
        "    elapsed_time = time.time() - start_at\n",
        "\n",
        "    return {\n",
        "        \"label\": label,            # Predicted sentiment label\n",
        "        \"score\": float(score),     # Predicted sentiment score\n",
        "        \"elapsed_time\": elapsed_time  # Elapsed time for prediction\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca38b1e6c9b5acfed7467de2cf02a78333108872",
        "id": "86RQiQcaOTQ0"
      },
      "cell_type": "code",
      "source": [
        "predict(\"I love the music\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e5fe647533be0148850de349fea6ef6f71303d1",
        "id": "fzFd9NMROTQ0"
      },
      "cell_type": "code",
      "source": [
        "predict(\"I hate the rain\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37064dffcc8920d34ccd54fac7c8b50e583a8269",
        "id": "_dYJZKi1OTQ0"
      },
      "cell_type": "code",
      "source": [
        "predict(\"i don't know what i'm doing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ee72e47f84b6dbc32e02a783de5ec1661f157e1",
        "id": "sZfme2_1OTQ0"
      },
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e920173eb05f04aecdd735bc5dff0f5be5f8d15",
        "id": "-zgmO7qZOTQ0"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Initialize lists to store predicted and true labels\n",
        "y_pred_1d = []\n",
        "y_test_1d = list(df_test.target)\n",
        "\n",
        "# Predict sentiment scores for the test data\n",
        "scores = model.predict(x_test, verbose=1, batch_size=8000)\n",
        "\n",
        "# Decode sentiment scores to labels\n",
        "y_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3575191bb425ab871f3f41e83812ee84bb7e595",
        "id": "BfTWRskDOTQ0"
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "\n",
        "    :param cm: ndarray, confusion matrix\n",
        "    :param classes: list, class labels\n",
        "    :param title: str, plot title\n",
        "    :param cmap: matplotlib colormap, color scheme for the plot\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize the confusion matrix\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=30)\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Define the tick marks for the plot\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n",
        "    plt.yticks(tick_marks, classes, fontsize=22)\n",
        "\n",
        "    # Add text annotations for each cell in the matrix\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add axis labels and set their font sizes\n",
        "    plt.ylabel('True label', fontsize=25)\n",
        "    plt.xlabel('Predicted label', fontsize=25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a57dc6f6211c144491a70f533225edfa95a2dc66",
        "id": "KyDszRCtOTQ1"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\n",
        "\n",
        "# Create a new figure with specified size\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "# Plot the confusion matrix using the defined function\n",
        "plot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title=\"Confusion matrix\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e23b957348dcc084249d3cc7538b972da471c2cd",
        "id": "f_kCGFwXOTQ1"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7fe05b7caa1c984ff1deb0be2f7c6bc043df9f5",
        "id": "zxmvDt-pOTQ1"
      },
      "cell_type": "code",
      "source": [
        "# Print the classification report\n",
        "print(classification_report(y_test_1d, y_pred_1d))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4eb300f0c6693a618587c7dcf32f77f5416cbfb9",
        "id": "CgWkWQ1yOTQ1"
      },
      "cell_type": "markdown",
      "source": [
        "### Accuracy Score"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cf76e6e09f8a60ed25947932b94c772eda44d23",
        "id": "i9jqxeRxOTQ1"
      },
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test_1d, y_pred_1d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f014c32f3833db282e1a075c526604f34e3158c",
        "id": "asPbTnCpOTQ1"
      },
      "cell_type": "markdown",
      "source": [
        "### Save model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b2b3ad5b592977b404acfa1c9ad303a62837255",
        "id": "XuMWcTpmOTQ1"
      },
      "cell_type": "code",
      "source": [
        "# Save the Keras model\n",
        "model.save(KERAS_MODEL)\n",
        "\n",
        "# Save the Word2Vec model\n",
        "w2v_model.save(WORD2VEC_MODEL)\n",
        "\n",
        "# Save the Tokenizer using pickle\n",
        "pickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\n",
        "\n",
        "# Save the LabelEncoder using pickle\n",
        "pickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}